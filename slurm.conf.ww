#
# See the slurm.conf man page for more information, but be
# sure to change them in {{.BuildSource}}
#
# 20250604 - brightview to warewulf update. (versions are year. stable debian still 3 years behind!)
# warewulf on debian12: slurmd 22.05.8-4+deb12u2 
# brightview cm on centos8: slurmd 19.05.5
ClusterName=warewulf
# Ordered List of Control Nodes
SlurmctldHost=cerebro2(10.141.255.253)
# warewulf example config uses below. but this is the name of the current node
#SlurmctldHost={{ .Id }}({{ .Ipaddr }})
#

SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#AuthType=auth/slurm # not avaliable in v22? (debian stable 2025)
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
# spool dir shouldbe in shared space
SlurmdSpoolDir=/opt/slurm/spool
SwitchType=switch/none
MpiDefault=none
# pids dont need to persist across reboots?
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
#ProctrackType=proctrack/pgid
ProctrackType=proctrack/cgroup
#PluginDir=
#CacheGroups=0 # 20250604  Parsing error at unrecognized key: CacheGroups
#FirstJobId=
ReturnToService=2
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
# removed from brightview for warewulf 20250604. example leaves undefined
#PrologSlurmctld=/cm/local/apps/cmd/scripts/prolog-prejob
#Prolog=/cm/local/apps/cmd/scripts/prolog
#Epilog=/cm/local/apps/cmd/scripts/epilog
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=task/cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFs=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log

#JobCompType=jobcomp/filetxt
#JobCompLoc=/cm/local/apps/slurm/var/spool/job_comp.log

#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
# AccountingStorageLoc=slurm_acct_db
# AccountingStoragePass=SLURMDBD_USERPASS
#
# 20250409WF - Nodes should share spare CPUs
SelectType=select/cons_res
# Core includes Hyperthreads, CPU does not
SelectTypeParameters=CR_Core_Memory
# for cons_res, OverSubscribe=YES same as no, but allows 'srun --oversubscribe'

# This section of this file was automatically generated by cmd. Do not edit manually!
# BEGIN AUTOGENERATED SECTION -- DO NOT REMOVE
# Server nodes
# AccountingStorageHost=master
# Nodes
# 20250411 - prefiously underspecified?
# Memory from 
#  sudo pdsh -w node[01-04,06-07] -- free -m |awk '(/Mem/){print $1,$3}'
# NB. node7 has 1/2 of the others!?
NodeName=cerebro2 Procs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 RealMemory=257905
NodeName=node[01-04,06] Procs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 RealMemory=257926 
NodeName=node07 Procs=16 Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 RealMemory=128902 
# Partitions
PartitionName=defq Default=YES MinNodes=1 AllowGroups=ALL PriorityJobFactor=1 PriorityTier=1 PreemptMode=OFF AllowAccounts=ALL AllowQos=ALL Nodes=node[01-04,06-07],cerebro2 OverSubscribe=YES
PartitionName=head Default=NO MinNodes=1 AllowGroups=ALL PriorityJobFactor=1 PriorityTier=1 PreemptMode=OFF AllowAccounts=ALL AllowQos=ALL Nodes=cerebro2 OverSubscribe=YES MaxCPUsPerNode=10
# Satesave
#StateSaveLocation=/cm/shared/apps/slurm/var/cm/statesave/slurm
# 20250604 - should be somewhere persistant?
StateSaveLocation=/opt/slurm/statesave/slurm
# Generic resources types
GresTypes=gpu
# 20250604WF -  fatal: The FastSchedule option has been removed. Please update your configuration.
#FastSchedule=0
